\chapter{Background}
\label{ch:background}

\section{Technology}
The experiment in this project thesis utilized a webserver and a search engine.
This section gives a brief summary of the technologies used.

NodeJS \footnote{\url{https://nodejs.org}} v7 were chosen as the webserver.
NodeJS were primarly chosen because the author of this project thesis had used it in multiple projects before.
Inside NodeJS lies the V8 \footnote{\url{https://developers.google.com/v8/}} JavaScript engine.
With NodeJS comes a rich package system delivered through NPM.

Elasticsearch \footnote{\url{https://www.elastic.co/products/elasticsearch}} v5 were utilized as the search engine.
At the time of writing Elasticsearch is one of the most popular search engine.
Elasticsearch is open source and built on top of Lucene \footnote{\url{https://lucene.apache.org/}}.
Lucene is the search engine itself,
and Elasticsearch provides functionality for distribution and a REST API interface.

\section{Search Engine}
This section describes how Elasticsearch scores its documents and is based on the documentation found on the website \cite{elasticsearch-scoring}.
The following subsections are needed to understand how the end result is determined, when using a term search in Elasticsearch.

[talk about IR book] \cite{ir-book}

\subsection{Term Frequency}
Term frequency is the number of times a term is mentioned in a document.
A document containing a term multiple times is probably more relevant than a document containing fewer occurences.
However, in this thesis a term is only present one time in each document, and the reason is described in greater detail in Chapter \ref{ch:approach}.
Term frequency calculation is given by Equation \ref{eq:term-frequency}.

\begin{cequation}[H]
	\begin{equation}
		\mathbf{tf} = \sqrt{frequency}
	\end{equation}
	\caption{Term frequency calculation in Elasticsearch}
  \label{eq:term-frequency}
\end{cequation}

\subsection{Inverse Document Frequency}
Inverse document frequency discribes how many times a term is present in all the documents.
Terms with high frequencies is often less relevant.
E.g. the terms "a" and "an" often appears in a sentence but should be the documents a high score.


\begin{cequation}[H]
	\begin{equation}
		\mathbf{idf} = 1 + \log{[\frac{numDocs}{docFrequency + 1}]}
	\end{equation}
	\caption{Inverse Document Frequency calculation in Elasticsearch}
  \label{eq:idf}
\end{cequation}

\subsection{Document Normalization}

\begin{cequation}[H]
	\begin{equation}
		\mathbf{normalization} = \frac{1}{\sqrt{numTerms}}
	\end{equation}
	\caption{Normalization}
  \label{eq:normalization}
\end{cequation}

\subsection{Document Score}
\label{sec:doc-score}
After calculating term frequency, inverse document frequency and document normalization all the factors are multiplied together.
A documents score in Elasticsearch is given by the Equation \ref{eq:document-score}.

\begin{cequation}[H]
	\begin{equation}
		\mathbf{documentScore} = tf \times idf \times normalization
	\end{equation}
	\caption{Final document score}
  \label{eq:document-score}
\end{cequation}

\subsection{Multiple term query}
The document score function described in Section \ref{sec:doc-score} is only used when searching for a single term.
When searching multiple terms the search Lucene are using multiple techniques: boolean model, TF/IDF and vector space model.

\url{http://lucene.apache.org/core/3_0_3/api/core/org/apache/lucene/search/Similarity.html}
\begin{cequation}[H]
	\begin{equation}
		\mathbf{score(q,d)} = coord(q,d) * queryNorm(q) * sum [tf(t in d) * idf(t)^2 * t.getBoost() * norm(t,d)]
	\end{equation}
	\caption{Lucene Scoring Function}
  \label{eq:scoring-function}
\end{cequation}

\section{Relevance Feedback}
The idea behind \textit{relevance feedback} is to use the result from the initial query to extract relevant information about the top-k documents.
Once the information is extracted, a new query is executed with extracted information.
Results from the second query is sent back to the user.
The assumption is that the second query returns documents which are more relevant to the user.

In \textit{Modern Information Retrieval} they define \textit{relevance feedback} as: "when the user explicitly provides information on relevant documents to a query,"
and \textit{query expansion} as: "when information related to the query is used to expand it" \cite[p. 177]{ir-book}.

\subsection{Explicit Feedback}

\subsection{Implicit Feedback}

\subsection{Pseudo-Relevance Feedback}
Retrieval of data to use relevance feedback requires interaction from the user.
Manually involving the user in the search is undesireable.
To avoid this an approach called pseudo-relevance feedback can be used.

Often the top-k documents are often used to find pseudo-relevance for query expansion.
However, the top-k documents are in many case not relevant, and thus not suitable data for query expansion \cite{pseudo-relevance-invalid}.
Section \ref{sec:query-expansion} describes a method to extract information from the top-k documents which is regarded as relevant information.

\section{Query Expansion}
\label{sec:query-expansion}
[Write about Query expansion]
The idea behind query expansion is to add more terms to the users query, and then use the extended query on the search engine.
According to literature a query expanded search does improve the results \cite[ch. 5]{ir-book}.
Even though research shows promissing results, query expansions require explicit information which in practice often is difficult to acquire.
On the other hand, according to Efron \cite{ir-hashtag} hashtags provides an excellent way to acquire the explicit information needed for query expansion.

There exists different method of query expansion and this thesis describes two techniques called Kullback-Leibler divergence and Rochio standard equation.
Only Kullback-Leibler is implemented in this project thesis, and the implementation is described in chapter \ref{ch:approach}.

\subsection{Kullback-Leibler Divergence}
Kullback-Leibler divergence measures how well distribution P represents the distrubution Q.

% Kullback-Leibler Distance Equation
\begin{cequation}[H]
	\begin{equation}
	    \label{equ:line}
		\mathbf{KL}_D[P(t), Q(t)] = P(t)*\log{[\frac{P(t)}{Q(t)}]}
	\end{equation}
	\caption{Kullback-Leibler Distance}
  \label{kl-distance}
\end{cequation}

\subsection{Rochio Standard Equation}
