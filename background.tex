\chapter{Background}
\label{ch:background}
This chapter explains important theory and techniques behind search engines.
First the technologies used in the implementation are explained.
Next the search engine is described and how it scores documents when quries are retrieved.
Lastly, relevance feedback is described and how it may be used with query expansion.

\section{Technology}
The experiment in this project report utilized a web server and a search engine as the backend.
This section gives a brief overview of the technologies used.

NodeJS\footnote{\url{https://nodejs.org}} v7 were chosen as the webserver.
NodeJS were chosen because the author has knowledge of the technology,
and it contains a rich package manager called NPM.
By utilizing open source libraries through NPM, more time could be spent implementing the algoritms for query expansion.
Inside NodeJS lies the V8\footnote{\url{https://developers.google.com/v8/}} JavaScript engine.

Elasticsearch\footnote{\url{https://www.elastic.co/products/elasticsearch}} v5 were utilized as the search engine.
At the time of writing Elasticsearch is a popular open source search engines, which has proven the ability to scale up to petabytes of data \cite{elasticsearch-scale}.
Elasticsearch is open source and built on top of Lucene\footnote{\url{https://lucene.apache.org/}}.
Lucene is the search engine itself,
and Elasticsearch provides functionality for distribution and a REST API interface.

\section{Search Engine}
A common approach for search engines is to use \textit{term frequency} (TF) and \textit{inverse document frequency} (IDF) to calculate a documents relevance based on a query.
Documents with the highest TF from a query are believed to be the most relevant.
On the other hand, the most common words are removed as they do not contain information about the topic.
TF and IDF alone is a simple model, and Elasticsearch uses a more sophisticated model.
Elasticsearch's document scoring model are described in the following subsections.
This section describes how Elasticsearch scores its documents and is based on the documentation found on the website \cite{elasticsearch-scoring}.

\subsection{Term Frequency}
Term frequency is the number of times a term is mentioned in a document.
A document containing a term multiple times is probably more relevant than a document containing fewer occurences.
However, in this work a term is only present one time in each document, and the reason is described in greater detail in Chapter \ref{ch:approach}.
Term frequency calculation is given by Equation \ref{eq:term-frequency}.

\begin{cequation}[H]
	\begin{equation}
		\mathbf{tf} = \sqrt{frequency}
	\end{equation}
	\caption{Term frequency calculation in Elasticsearch}
  \label{eq:term-frequency}
\end{cequation}

\subsection{Inverse Document Frequency}
Inverse document frequency describes how many times a term is present in all the documents.
Terms with high frequencies is often less relevant.
E.g. the terms "a" and "an" often appears in a sentence but should not be given a high score even though they appear many times.

\begin{cequation}[H]
	\begin{equation}
		\mathbf{idf} = 1 + \log{[\frac{numDocs}{docFrequency + 1}]}
	\end{equation}
	\caption{Inverse Document Frequency calculation in Elasticsearch}
  \label{eq:idf}
\end{cequation}

\subsection{Document Normalization}
A title field is likely to be shorter compared to a description field.
As a result, the description field possibly contains more mentions of a given term.
To account for longer fields document normalization is used.
Elasticsearch's implementation is illustrated in equation \ref{eq:normalization}.

\begin{cequation}[H]
	\begin{equation}
		\mathbf{normalization} = \frac{1}{\sqrt{numTerms}}
	\end{equation}
	\caption{Normalization}
  \label{eq:normalization}
\end{cequation}

\subsection{Document Score}
\label{sec:doc-score}
After calculating term frequency, inverse document frequency and document normalization all the factors are multiplied together.
A documents score in Elasticsearch is given by the Equation \ref{eq:document-score}.

\begin{cequation}[H]
	\begin{equation}
		\mathbf{documentScore} = tf \times idf \times normalization
	\end{equation}
	\caption{Final document score}
  \label{eq:document-score}
\end{cequation}

\subsection{Multiple term query}
The document score function described in Section \ref{sec:doc-score} is only used when searching for a single term.
While a term query contaning multiple terms requires a combination of techniques.
A multiple term query inside Lucene combines the following techniques:
boolean model, TF/IDF and vector space model.
Equation \ref{eq:scoring-function} shows how each document are scored against a multiterm query.
Table \ref{tbl:scoring-function} explaines each variable in equation \ref{eq:scoring-function}.
A more in depth explenation is available at Elasticsearch's documentation\footnote{\url{https://www.elastic.co/guide/en/elasticsearch/guide/current/practical-scoring-function.html}}.

\begin{cequation}
	\begin{equation}
		\begin{aligned}
			\mathbf{score(q,d)} = & coord(q,d) \times queryNorm(q) \\
														& \times \sum tf(t in d) \times idf(t)^2 \times t.getBoost() \times norm(t,d)
		\end{aligned}
	\end{equation}
	\caption{Equation for scoring documents when searching with multiple terms. Each variable are described in table \ref{tbl:scoring-function}}
  \label{eq:scoring-function}
\end{cequation}

\begin{table}
		\centering
    \begin{tabular}{|l|l|}
    \hline
		\multicolumn{1}{|c|}{\bfseries Variable} & \multicolumn{1}{c|}{\bfseries Description} \\ \hline
    \textit{t}         & term                           		\\ \hline
    \textit{d}         & document                       		\\ \hline
    \textit{q}         & query                          		\\ \hline
		\textit{score}     & document score from a given query	\\ \hline
    \textit{coord}     & coordination factor            		\\ \hline
    \textit{queryNorm} & query normalization factor     		\\ \hline
    \textit{tf}        & term frequency                 		\\ \hline
    \textit{idf}       & inverse document frequency     		\\ \hline
    \textit{getBoost}  & boost factor used on the query 		\\ \hline
    \textit{norm}      & document normalization factor  		\\ \hline
    \end{tabular}
		\caption{Variable descriptions for equation \ref{eq:scoring-function}}
		\label{tbl:scoring-function}
\end{table}

\section{Relevance Feedback}
The idea behind \textit{relevance feedback} is to use the result from the initial query to extract relevant information about the top-k documents.
Once the information is extracted, a new query is executed with extracted information.
Results from the second query is sent back to the user.
The assumption is that the second query returns documents which are more relevant to the user.

In \cite{ir-book} the authors define \textit{relevance feedback} as: "when the user explicitly provides information on relevant documents to a query,"
and \textit{query expansion} as: "when information related to the query is used to expand it" \cite[p. 177]{ir-book}.
In other words to use \textit{relevance feedback} input from the user is needed.
For example the user could be given the task to mark each whether each document in the result were relevant or not.
For \textit{query expansion} information like position and hashtags may be used to expand the query.
A more detailed explanation of query expansion is described in section \ref{sec:query-expansion}.

\textit{Relevance feedback} is divided into three main categories \textit{explicit feedback}, \textit{implicit feedback} and \textit{pseude-relevance feedback},
and is introduced in the next two subsections.

\subsection{Explicit vs Implicit Feedback}
\textit{Explicit feedback} data are retrieved directly from user interaction.
An example would let the user mark whether the results were relevant or not.
Another approach is to use data from a user search.
If a user clicks a search result, the result may be regarded as relevant.
Even though the result may not be relevant, it is a good indication.

The problem with explicit feedback, is that it requires interaction from the user.
\textit{Implicit feedback} on the other hand, doesn't require any involvement from the user.
Examples of implicit user data are:
collect which documents are view or not from a result, and measure time spent viewing a document.

\subsection{Pseudo-Relevance Feedback}
Retrieval of data to use relevance feedback requires either explicit or implicit user interaction.
Manually involving the user in the search is undesireable.
To avoid this an approach called pseudo-relevance feedback can be used.
Using implicit feedback requires a system which does the data collection and post process the information.
Pseudo-relevance on the other hand, uses information from the first search, which results in a simpler implementation.

Often the top-k documents are used to find pseudo-relevance for query expansion.
However, the top-k documents are in many cases not relevant, and thus not suitable data for query expansion \cite{pseudo-relevance-invalid}.
Section \ref{sec:query-expansion} describes a method to extract information from the top-k documents which is regarded as relevant information.

\section{Query Expansion}
\label{sec:query-expansion}
When a user search with the query "Super Bowl" the day after the sport event has taken place,
it is likely that the user wants information about the event from the day before.
The query "Super Bowl" is likely to also return documents from previous years.
If the search engine could be able to notice that recent documents also contains the term "2016,"
the extra term could be added to the query.
The new query "Super Bowl 2016" is likely to rank documents from this year's superbowl higher, as a result of the extra term.
This technique is called query expansion.

The idea behind query expansion is to add more terms to the users query, and then use the extended query on the search engine.
According to literature a query expanded search does improve the results \cite[ch. 5]{ir-book}.
Even though research shows promissing results, query expansions require explicit information which in practice often is difficult to acquire.
On the other hand, according to Efron \cite{ir-hashtag} hashtags provides an excellent way to acquire the explicit information needed for query expansion.

There exists different method of query expansion and this report describes two techniques called Kullback-Leibler divergence and Rochio standard equation.
Kullback-Leibler is implemented in this project report, and the implementation is described in chapter \ref{ch:approach}.

\subsection{Kullback-Leibler Divergence}
\textit{Kullback-Leibler divergence} (KL) measures how well distribution P(t) represents the distribution Q(t).
The variables in distribution P(t) and Q(t) is explained in the bullet points bellow.

\begin{itemize}
	\item \textit{numberOfTimesInTopKDocuments} is the number of times a term is present in the top-k documents
	\item \textit{numberOfTermsInTopKDocuments} is the number of terms in total in the top-k documents
	\item \textit{totalNumberOfTimesInCollection} is the total number of times a term is present in the data collection
	\item \textit{totalNumberOfTermsInCollection} is the total number of terms in the data collection
\end{itemize}

Equation \ref{eq:kl-distribution-p} explains how to calculate the distribution P(t),
and equation \ref{eq:kl-distribution-q} explains how to calculate distribution Q(t).

% Kullback-Leibler P distribution
\begin{cequation}[H]
	\begin{equation}
		\mathbf{P} = \frac{numberOfTimesInTopKDocuments}{numberOfTermsInTopKDocuments}
	\end{equation}
	\caption{}
  \label{eq:kl-distribution-p}
\end{cequation}

% Kullback-Leibler Q distribution
\begin{cequation}[H]
	\begin{equation}
		\mathbf{Q} = \frac{totalNumberOfTimesInCollection}{totalNumberOfTermsInCollection}
	\end{equation}
	\caption{}
  \label{eq:kl-distribution-q}
\end{cequation}

Computing the Kullback-Leibler divergence for a term is given by equation \ref{eq:kl-divergence}.

% Kullback-Leibler Distance Equation
\begin{cequation}[H]
	\begin{equation}
		\mathbf{KL}_D[P(t), Q(t)] = P(t)*\log{[\frac{P(t)}{Q(t)}]}
	\end{equation}
	\caption{Kullback-Leibler Divergence}
  \label{eq:kl-divergence}
\end{cequation}

\subsubsection{Kullback-Leibler Divergence Example}
To illustrate how KL score is calculated an example for the search term "sky" is shown.
The expanded query may consist of up to 5 terms.
To keep this example short only the top 5 terms is calculated and the term "sky" is excluded from the calculations.

Table \ref{tbl:kl-counts} contains information extracted using the dataset described in section \ref{sec:dataset}.
Using equation \ref{eq:kl-divergence} and the information in the KL score for each term are calculated and is shown in table \ref{tbl:kl-score}.
The table \ref{tbl:kl-score} lists the terms descending according to their score.
From before we have the term "sky" and we may use 4 more to complete the expansion.
The top 4 terms are "clouds", "2016", "blue" and "water".
Which results in an expanded query containg the terms: "sky", "clouds", "2016", "blue" and "water".


\begin{table}
	\centering
    \begin{tabular}{|l|p{30mm}|p{30mm}|p{30mm}|p{30mm}|}
		\hline
    \textbf{Term}   & \textbf{numberOf- \newline TimesIn- \newline TopK- \newline Documents} &
		\textbf{numberOf- \newline TermsIn- \newline TopK- \newline Documents} &
		\textbf{totalNumber- \newline OfTimesIn- \newline Collection} &
		\textbf{totalNumber- \newline OfTermsIn- \newline Collection} \\ \hline
    blue   & 1                            & 42                           & 14                             & 298,962                         \\ \hline
    2016   & 2                            & 42                           & 143                            & 298,962                         \\ \hline
    clouds & 3                            & 42                           & 31                             & 298,962                         \\ \hline
    sea    & 1                            & 42                           & 34                             & 298,962                         \\ \hline
    water  & 1                            & 42                           & 24                             & 298,962                         \\ \hline
\end{tabular}
	\caption{The returned numbers for each of the top 5 terms excluding the term "sky"}
	\label{tbl:kl-counts}
\end{table}

\begin{table}
	\centering
    \begin{tabular}{|l|l|}
		\hline
    \textbf{Term}   & \textbf{Score}   \\ \hline
    clouds & 0.46678 \\ \hline
    2016   & 0.21908 \\ \hline
    blue   & 0.14836 \\ \hline
    water  & 0.13553 \\ \hline
    sea    & 0.12723 \\ \hline
    \end{tabular}
	\caption{KL divergence score of each term}
	\label{tbl:kl-score}
\end{table}
