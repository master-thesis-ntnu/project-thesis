\chapter{Background}
\label{ch:background}

\section{Technology}
The experiment in this project thesis utilized a webserver and a search engine.
This section gives a brief summary of the technologies used.

NodeJS \footnote{\url{https://nodejs.org}} v7 were chosen as the webserver.
NodeJS were primarly chosen because the author of this project thesis had used it in multiple projects before.
Inside NodeJS lies the V8 \footnote{\url{https://developers.google.com/v8/}} JavaScript engine.
With NodeJS comes a rich package system delivered through NPM.

Elasticsearch \footnote{\url{https://www.elastic.co/products/elasticsearch}} v5 were utilized as the search engine.
At the time of writing Elasticsearch is one of the most popular search engine.
Elasticsearch is open source and built on top of Lucene \footnote{\url{https://lucene.apache.org/}}.
Lucene is the search engine itself,
and Elasticsearch provides functionality for distribution and a REST API interface.

\section{Search Engine}
This section describes how Elasticsearch scores its documents and is based on the documentation found on the website \cite{elasticsearch-scoring}.
The following subsections are needed to understand how the end result is determined, when using a term search in Elasticsearch.

[talk about IR book] \cite{ir-book}

\subsection{Term Frequency}
Term frequency is the number of times a term is mentioned in a document.
A document containing a term multiple times is probably more relevant than a document containing fewer occurences.
However, in this thesis a term is only present one time in each document, and the reason is described in greater detail in Chapter \ref{ch:approach}.
Term frequency calculation is given by Equation \ref{eq:term-frequency}.

\begin{cequation}[H]
	\begin{equation}
		\mathbf{tf} = \sqrt{frequency}
	\end{equation}
	\caption{Term frequency calculation in Elasticsearch}
  \label{eq:term-frequency}
\end{cequation}

\subsection{Inverse Document Frequency}
Inverse document frequency discribes how many times a term is present in all the documents.
Terms with high frequencies is often less relevant.
E.g. the terms "a" and "an" often appears in a sentence but should be the documents a high score.


\begin{cequation}[H]
	\begin{equation}
		\mathbf{idf} = 1 + \log{[\frac{numDocs}{docFrequency + 1}]}
	\end{equation}
	\caption{Inverse Document Frequency calculation in Elasticsearch}
  \label{eq:idf}
\end{cequation}

\subsection{Document Normalization}

\begin{cequation}[H]
	\begin{equation}
		\mathbf{normalization} = \frac{1}{\sqrt{numTerms}}
	\end{equation}
	\caption{Normalization}
  \label{eq:normalization}
\end{cequation}

\subsection{Document Score}
\label{sec:doc-score}
After calculating term frequency, inverse document frequency and document normalization all the factors are multiplied together.
A documents score in Elasticsearch is given by the Equation \ref{eq:document-score}.

\begin{cequation}[H]
	\begin{equation}
		\mathbf{documentScore} = tf \times idf \times normalization
	\end{equation}
	\caption{Final document score}
  \label{eq:document-score}
\end{cequation}

\subsection{Multiple term query}
The document score function described in Section \ref{sec:doc-score} is only used when searching for a single term.
When searching multiple terms the search Lucene are using multiple techniques: boolean model, TF/IDF and vector space model.


\url{http://lucene.apache.org/core/3_0_3/api/core/org/apache/lucene/search/Similarity.html}
\begin{cequation}[H]
	\begin{equation}
		\mathbf{score(q,d)} = coord(q,d) * queryNorm(q) * sum [tf(t in d) * idf(t)^2 * t.getBoost() * norm(t,d)]
	\end{equation}
	\caption{Lucene Scoring Function}
  \label{eq:scoring-function}
\end{cequation}

\section{Query Expansion}
\label{sec:query-expansion}
[Write about Query expansion]
The idea behind query expansion is to add more terms to the users query, and then use the extended query on the search engine.
According to literature a query expanded search does improve the results \cite[ch. 5]{ir-book}.
Even though research shows promissing results, query expansions require explicit information which in practice often is difficult to acquire.
But according to Efron \cite{ir-hashtag} hashtags provides an excellent way of acquire the explicit information needed for query expansions

There exists different method of query expansion and this thesis describes two techniques called Kullback-Leibler divergence and Rochio standard equation.

\subsection{Kullback-Leibler Divergence}
Kullback-Leibler divergence measures how well distribution P represents the distrubution Q.

% Kullback-Leibler Distance Equation
\begin{cequation}[H]
	\begin{equation}
	    \label{equ:line}
		\mathbf{KL}_D[P(t), Q(t)] = P(t)*\log{[\frac{P(t)}{Q(t)}]}
	\end{equation}
	\caption{Kullback-Leibler Distance}
  \label{kl-distance}
\end{cequation}

\subsection{Rochio Standard Equation}
