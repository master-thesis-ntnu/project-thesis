\chapter{Background}
\label{ch:background}
This chapter explains important theory and techniques behind search engines.
First the technologies used in the implementation are explained.
Next the search engine is described and how it scores documents when quries are retrieved.
Lastly, relevance feedback is described and how it may be used with query expansion.

\section{Technology}
The experiment in this project report utilized a web server and a search engine as the backend.
This section gives a brief overview of the technologies used.

NodeJS \footnote{\url{https://nodejs.org}} v7 were chosen as the webserver.
NodeJS were chosen because the author of this project report had used it in multiple projects before,
and it consists of a rich package system called NPM.
By utilizing open source libraries through NPM, more time could be spent implementing the algoritms for query expansion.
Inside NodeJS lies the V8 \footnote{\url{https://developers.google.com/v8/}} JavaScript engine.

Elasticsearch \footnote{\url{https://www.elastic.co/products/elasticsearch}} v5 were utilized as the search engine.
At the time of writing Elasticsearch is a popular open source search engines, which has proven the ability to scale up to petabytes of data \cite{elasticsearch-scale}.
Elasticsearch is open source and built on top of Lucene \footnote{\url{https://lucene.apache.org/}}.
Lucene is the search engine itself,
and Elasticsearch provides functionality for distribution and a REST API interface.

\section{Search Engine}
A common approach for search engines is to use \textit{term frequency} (TF) and \textit{inverse document frequency} (IDF) to calculate a documents relevance based on a query.
Documents with the highest TF from a query are believed to be the most relevant.
On the other hand, the most common words are removed as they do not contain information about the topic.
TF and IDF alone is a simple model, and Elasticsearch uses a more sofisticated model.
Elasticsearch's document scoring model are described in the following subsections.
This section describes how Elasticsearch scores its documents and is based on the documentation found on the website \cite{elasticsearch-scoring}.

\subsection{Term Frequency}
Term frequency is the number of times a term is mentioned in a document.
A document containing a term multiple times is probably more relevant than a document containing fewer occurences.
However, in this work a term is only present one time in each document, and the reason is described in greater detail in Chapter \ref{ch:approach}.
Term frequency calculation is given by Equation \ref{eq:term-frequency}.

\begin{cequation}[H]
	\begin{equation}
		\mathbf{tf} = \sqrt{frequency}
	\end{equation}
	\caption{Term frequency calculation in Elasticsearch}
  \label{eq:term-frequency}
\end{cequation}

\subsection{Inverse Document Frequency}
Inverse document frequency describes how many times a term is present in all the documents.
Terms with high frequencies is often less relevant.
E.g. the terms "a" and "an" often appears in a sentence but should not be given a high score even though they appear many times.

\begin{cequation}[H]
	\begin{equation}
		\mathbf{idf} = 1 + \log{[\frac{numDocs}{docFrequency + 1}]}
	\end{equation}
	\caption{Inverse Document Frequency calculation in Elasticsearch}
  \label{eq:idf}
\end{cequation}

\subsection{Document Normalization}
A title field is likely to be shorter compared to a description field.
As a result description is possibly contains more mentions of a given term.
To account for longer fields document normalization is used.
Elasticsearch's implementation is illustrated in equation \ref{eq:normalization}.

\begin{cequation}[H]
	\begin{equation}
		\mathbf{normalization} = \frac{1}{\sqrt{numTerms}}
	\end{equation}
	\caption{Normalization}
  \label{eq:normalization}
\end{cequation}

\subsection{Document Score}
\label{sec:doc-score}
After calculating term frequency, inverse document frequency and document normalization all the factors are multiplied together.
A documents score in Elasticsearch is given by the Equation \ref{eq:document-score}.

\begin{cequation}[H]
	\begin{equation}
		\mathbf{documentScore} = tf \times idf \times normalization
	\end{equation}
	\caption{Final document score}
  \label{eq:document-score}
\end{cequation}

\subsection{Multiple term query}
The document score function described in Section \ref{sec:doc-score} is only used when searching for a single term.
When searching multiple terms the search Lucene \footnote{\url{http://lucene.apache.org/core/3_0_3/api/core/org/apache/lucene/search/Similarity.html}}
are using multiple techniques: boolean model, TF/IDF and vector space model.
Equation \ref{eq:scoring-function} shows how each document are scored against a multiterm query.
Table \ref{tbl:scoring-function} explaines each variable in equation \ref{eq:scoring-function}.
A more in depth explenation is available at Elasticsearch's documentation \footnote{\url{https://www.elastic.co/guide/en/elasticsearch/guide/current/practical-scoring-function.html}}.

\begin{cequation}
	\begin{equation}
		\begin{aligned}
			\mathbf{score(q,d)} = & coord(q,d) \times queryNorm(q) \\
														& \times \sum tf(t in d) \times idf(t)^2 \times t.getBoost() \times norm(t,d)
		\end{aligned}
	\end{equation}
	\caption{Equaction for scoring documents when searching with multiple terms. Each variable are described in table \ref{tbl:scoring-function}}
  \label{eq:scoring-function}
\end{cequation}

\begin{table}
		\centering
    \begin{tabular}{|l|l|}
    \hline
		\multicolumn{1}{|c|}{\bfseries Variable} & \multicolumn{1}{c|}{\bfseries Description} \\ \hline
    \textit{t}         & term                           		\\ \hline
    \textit{d}         & document                       		\\ \hline
    \textit{q}         & query                          		\\ \hline
		\textit{score}     & document score from a given query	\\ \hline
    \textit{coord}     & coordination factor            		\\ \hline
    \textit{queryNorm} & query normalization factor     		\\ \hline
    \textit{tf}        & term frequency                 		\\ \hline
    \textit{idf}       & inverse document frequency     		\\ \hline
    \textit{getBoost}  & boost factor used on the query 		\\ \hline
    \textit{norm}      & document normalization factor  		\\ \hline
    \end{tabular}
		\caption{Variable descriptions for equation \ref{eq:scoring-function}}
		\label{tbl:scoring-function}
\end{table}

\section{Relevance Feedback}
The idea behind \textit{relevance feedback} is to use the result from the initial query to extract relevant information about the top-k documents.
Once the information is extracted, a new query is executed with extracted information.
Results from the second query is sent back to the user.
The assumption is that the second query returns documents which are more relevant to the user.

In \cite{ir-book} the authors define \textit{relevance feedback} as: "when the user explicitly provides information on relevant documents to a query,"
and \textit{query expansion} as: "when information related to the query is used to expand it" \cite[p. 177]{ir-book}.
In other words to use \textit{relevance feedback} input from the user is needed.
For example the user could be given the task to mark each whether each document in the result were relevant or not.
For \textit{query expansion} information like position and hashtags may be used to expand the query.

\textit{Relevance feedback} is divided into three main categories \textit{explicit feedback}, \textit{implicit feedback} and \textit{pseude-relevance feedback}.

\subsection{Explicit vs Implicit Feedback}
\textit{Explicit feedback} data are retrieved directly from user interaction.
An example would let the user mark whether the results were relevant or not.
Another approach is to use data from a user search.
If a user clicks a search result, the result may be regarded as relevant.
Even though the result may not be relevant, it is a good indication.

The problem with explicit feedback, is that it requires interaction from the user.
\textit{Implicit feedback} on the other hand, doesn't require any involvement from the user.
Examples of implicit user data are:
collect which documents are view or not from a result, and measure time spent viewing a document.

\subsection{Pseudo-Relevance Feedback}
Retrieval of data to use relevance feedback requires either explicit or implicit user interaction.
Manually involving the user in the search is undesireable.
To avoid this an approach called pseudo-relevance feedback can be used.

Often the top-k documents are often used to find pseudo-relevance for query expansion.
However, the top-k documents are in many case not relevant, and thus not suitable data for query expansion \cite{pseudo-relevance-invalid}.
Section \ref{sec:query-expansion} describes a method to extract information from the top-k documents which is regarded as relevant information.
[too short][??]

\section{Query Expansion}
\label{sec:query-expansion}
[Write about Query expansion][??]
The idea behind query expansion is to add more terms to the users query, and then use the extended query on the search engine.
According to literature a query expanded search does improve the results \cite[ch. 5]{ir-book}.
Even though research shows promissing results, query expansions require explicit information which in practice often is difficult to acquire.
On the other hand, according to Efron \cite{ir-hashtag} hashtags provides an excellent way to acquire the explicit information needed for query expansion.

There exists different method of query expansion and this report describes two techniques called Kullback-Leibler divergence and Rochio standard equation.
Kullback-Leibler is implemented in this project report, and the implementation is described in chapter \ref{ch:approach}.

\subsection{Kullback-Leibler Divergence}
\textit{Kullback-Leibler divergence} (KL) measures how well distribution P(t) represents the distribution Q(t).
The variables in distribution P(t) and Q(t) is explained in the bullet points bellow.

\begin{itemize}
	\item \textit{numberOfTimesInTopKDocuments} is the number of times a term is given present in the top k documents
	\item \textit{numberOfTermsInTopKDocuments} is the number of terms in total in the top k documents
	\item \textit{totalNumberOfTimesInCollection} is the total number of times a term is present in the data collection
	\item \textit{totalNumberOfTermsInCollection} is the total number of terms in the data collection
\end{itemize}

Equation \ref{eq:kl-distribution-p} explains how to calculate the distribution P(t),
and equation \ref{eq:kl-distribution-q} explains how to calculate distribution Q(t).

% Kullback-Leibler P distribution
\begin{cequation}[H]
	\begin{equation}
		\mathbf{P} = \frac{numberOfTimesInTopKDocuments}{numberOfTermsInTopKDocuments}
	\end{equation}
	\caption{}
  \label{eq:kl-distribution-p}
\end{cequation}

% Kullback-Leibler Q distribution
\begin{cequation}[H]
	\begin{equation}
		\mathbf{Q} = \frac{totalNumberOfTimesInCollection}{totalNumberOfTermsInCollection}
	\end{equation}
	\caption{}
  \label{eq:kl-distribution-q}
\end{cequation}

Computing the Kullback-Leibler distance for a term is given by equation \ref{eq:kl-distance}.

% Kullback-Leibler Distance Equation
\begin{cequation}[H]
	\begin{equation}
		\mathbf{KL}_D[P(t), Q(t)] = P(t)*\log{[\frac{P(t)}{Q(t)}]}
	\end{equation}
	\caption{Kullback-Leibler Distance}
  \label{eq:kl-distance}
\end{cequation}

\subsubsection{Kullback-leibler Divergence}
To illustrate how KL is calculated an example for the search term "sky" is shown.
To keep this example short only the top 5 terms is calculated.

[add example][??]
