\chapter{Evaluation}
\label{ch:evaluation}
The evaluation chapter describes the experimental setup, the dataset and the results.

\section{Dataset}
\label{sec:dataset}
The dataset consists of data from the Flickr API \footnote{\url{https://www.flickr.com/services/api/}}.
Flicker provides an API enpoint which containts data of the latest images.
As the API limits the number of requests per hour, the data had to be collected over an extended period of time.
The test data was gathered from this endpoint over a time period of about 4 weeks.

The internal representation in Elasticsearch of the data from Flickr can be seen in appendix \ref{ap:flickr-data}.
This data contains 115,105 photos and 298,962 tags, and the photo index takes up 152.3 MB in Elasticsearch.
Dynamic mapping was used for defining the fields for the internal representation in Elasticsearch.
On a production environment the most optimal setting would be to use Elasticsearch's explicit mapping.
A single index called \textit{photos} was used to hold the photo data.

\section{Experimental Setup}
The experimental setup contains of two main parts, a client and a web server.
A laptop was used as the client and a desktop as the web server.
Both computer was placed on the some local network.
The hardware specification of the laptop was: Intel Core i7 2.50 GHz x 4 and 8 GB DDR3L RAM,
and the desktop had the following specifications: Intel Core i5 3.40 GHz x 4 and 16 GB of DDR3 RAM.

The web server contained both Elasticsearch and the NodeJS web server.
Elasticsearch's configuration was set to default settings,
except that the memory heap size was changed from the default value of 2 GB to 4 GB.
Elasticsearch v5.0.0 was used during the experiment.

As mentioned earlier NodeJS was configured to use the internal clustering functionality.
Using a desktop with 4 cores the web server were running 4 instances of NodeJS.
NodeJS accessed Elasticsearch through the available HTTP REST API \cite{elasticsearch-rest-api}.

\subsection{Performance Metrics}
The performance metric used in this experiment was latency.
Latency was measure from when the request left the laptop to the response retrieved.
The command line tool called \texttt{ab} \cite{apache-benchmark} was used to evaluate the latency.


%Figure \ref{fig:experiment-setup} illustrates the experiment setup.
%\begin{figure}[h!]
%  \centering \includegraphics[width=0.9\linewidth]{img/experiment_setup.png}
%  \caption{Experiment setup}
%  \label{fig:experiment-setup}
%\end{figure}

%\begin{table}[h]
%    \centering
%    \begin{tabular}{c|c}
%      \textbf{Component} & \textbf{Model} \\ \hline
%      CPU       & Intel Core i5-4670 3.4 GHz x 4        \\ \hline
%      RAM       & Crucial BallistixSport 2x8GB 1600 MHz \\ \hline
%      SSD       & OCZ Vertex 4 256 GB                   \\ \hline
%    \end{tabular}
%    \caption{Hardware components of the computer running the experiments}
%    \label{tbl:hardware}
%\end{table}

\section{Results}
Elasticsearch caches frequently used aggregations.
To utilize this feature of Elasticsearch the same query was sent each time.
The query contained the top 5 most popular tags from the dataset: \textit{"square" "iphoneography" "squareformat" "instagramapp" "uploaded:by=instagram"}.
Each request was sent 1000 times, and the average and median latency was calculated.

Three different types of tests were conducted: query without expansion query with expansion and a query were the number of top-k documents varied.
The first two tests varied the number of concurrent requests from 1 to 150.
While the last request had a fixed number of concurrent request at 10,
but varied the number of top-k documents from 10 to 200.

To measure the impact of query expansion a baseline is needed.
The baseline results where retrieved by using the same terms but withouth the query expansion.
Table \ref{tbl:baseline} displays results from the baseline test.
From the table we can see that the backend were able to handle between 50 and 100 concurrent requests and still meet the interactivity requirement.

The test results from the requests with query expansion are listed in table \ref{tbl:query-expansion}.
In the table one can see that query expansion have a clear impact on the latency, even with only one concurrent request.
Nevertheless, the query expansion is able to meet the interactivity with up to 20 concurrent requests.
Compared to the results from the baseline,
the query expansion implementation is able to handle less than half the number of request.

On the last test the number of top-k documents were varied and the results are listed in table \ref{tbl:query-expansion-topk}.
To make sure the web server wasn't the bottleneck, the number of concurrent requests were fixed at 10.
As k increases from 10 to 200 the latency increases.
From the table one can see that varying the k has little impact on the performance.
The latency increased from below 50 ms to between 70 and 80 ms, which is still within the interactivity requirement.
The increased latency is most likely due to the query expansion step on the web server,
the increased data size from Elasticsearch and from Elasticsearch itself.

\input{result-tables}

\section{Discussion}
Figure \ref{fig:sequence-diagram-search-master} shows the sequence diagram for Rudihagen's query expansion solution using KL.
On the figure 4 round trips are required from the webserver to the database and the search engine.
The implementation discussed in Chapter \ref{ch:approach} describes a solution to decrease the number of round trips from 4 to 2.
In Chapter \ref{ch:evaluation} the performance is greatly increased.

\begin{figure}[h!]
\centering \includegraphics[width=0.9\linewidth]{img/sequence-diagram-search-master-thesis.png}
\caption{Sequence diagram from Rudihagen's master thesis implementation of query expansion using KL \cite[p. 37]{master-thesis}.}
\label{fig:sequence-diagram-search-master}
\end{figure}

Even though the latency was greatly reduced there are a few disclamers.
Firstly both the webserver and the search engine ran on the same computer.
As a result the round trip time was almost negligible, which is often not the case in a real world environment.
To give more realistic measurements, the solution should be tested on a cloud solution where the webserver and the search engine doesn't run on the same physical server.
